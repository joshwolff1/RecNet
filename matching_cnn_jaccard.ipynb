{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "matching_cnn_jaccard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7lchdAN5rFO",
        "colab_type": "text"
      },
      "source": [
        "# Mount Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyeBlJaz1SXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce62198e-d32e-468a-c26e-d46811043922"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/Shared\\ drives/CS230/datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/Shared drives/CS230/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DyMPtm6Aa_",
        "colab_type": "text"
      },
      "source": [
        "# Load required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2dIxMb4ztt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0) # Reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yatOaC8DJU0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJDN3HomJVwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6d603ab-8aaa-4d28-b0c1-959e56992857"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-XNynFLKwuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2455c716-4679-4562-f73a-1be10b40e126"
      },
      "source": [
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 17629093523220778877, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16019287805258879659\n",
              " physical_device_desc: \"device: XLA_CPU device\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va_-fsjrHeKN",
        "colab_type": "text"
      },
      "source": [
        "# Set Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnlgX-xAFhkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prot_lookup = {'A' : 0, 'C' : 1, 'D' : 2, 'E' : 3, \n",
        "               'F' : 4, 'G' : 5, 'H' : 6, 'I' : 7, \n",
        "               'K' : 8, 'L' : 9, 'M' : 10, 'N' : 11, \n",
        "               'P' : 12, 'Q' : 13, 'R' : 14, 'S' : 15, \n",
        "               'T' : 16, 'V' : 17, 'W' : 18, 'Y' : 19}\n",
        "attx_lookup = {'A' : 0, 'T' : 1, 'G' : 2, 'C' : 3}\n",
        "max_prot_len, max_attx_len = 100, 150\n",
        "prot_dim, attx_dim, k = 20, 4, 4\n",
        "aa_list = list(prot_lookup.keys())\n",
        "\n",
        "def gen_kgrams(k):\n",
        "\n",
        "  grams = []\n",
        "  if (k > 1):\n",
        "    lower_grams = gen_kgrams(k - 1)\n",
        "  else:\n",
        "    lower_grams = ['']\n",
        "  for p in aa_list:\n",
        "    for gram in lower_grams:\n",
        "      grams.append(p + gram)\n",
        "  return grams\n",
        "\n",
        "grams = gen_kgrams(k)\n",
        "\n",
        "idx_maps = []\n",
        "for j in range(max_prot_len):\n",
        "  idx_map = {}\n",
        "  random.shuffle(grams)\n",
        "  for idx, gram in enumerate(grams):\n",
        "    idx_map[gram] = idx\n",
        "  idx_maps.append(idx_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTpWvSl5Y6aO",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hznyCDuFY_54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_set(line_list):\n",
        "\n",
        "  attx_set_1h = np.zeros((len(line_list), max_attx_len, attx_dim, 1))\n",
        "  attx_set = np.zeros((len(line_list), max_attx_len))\n",
        "  prot_set = np.zeros((len(line_list), max_prot_len))\n",
        "  label_set = np.zeros((len(line_list), 1))\n",
        "  \n",
        "  for i in tqdm(range(len(line_list))):\n",
        "    line = line_list[i]\n",
        "    _, _, attx, prot, label = line.strip().split()\n",
        "    label_set[i, 0] = int(label)\n",
        "    for j in range(len(attx)):\n",
        "      attx_set_1h[i, j, attx_lookup[attx[j]], 0] = 1\n",
        "      attx_set[i, j] = attx_lookup[attx[j]]\n",
        "    grams = [prot[i:i+k] for i in range(len(prot)-(k-1))]\n",
        "    for j in range(max_prot_len):\n",
        "      prot_set[i, j] = min(idx_maps[j][gram] for gram in grams)\n",
        "  \n",
        "  print('Pos Frac: ', sum(label_set) / len(line_list))\n",
        "  return attx_set_1h, attx_set, prot_set, label_set \n",
        "    \n",
        "train_path = 'set2/attx_protein_binding/train_attx_protein_binding.tsv'\n",
        "test_path = 'set2/attx_protein_binding/test_attx_protein_binding.tsv'\n",
        "dev_path = 'set2/attx_protein_binding/dev_attx_protein_binding.tsv'\n",
        "\n",
        "with open(train_path) as f:\n",
        "  train_list = f.readlines()\n",
        "\n",
        "with open(test_path) as f:\n",
        "  test_list = f.readlines()\n",
        "\n",
        "with open(dev_path) as f:\n",
        "  dev_list = f.readlines()\n",
        "\n",
        "print('Generating Train Set')\n",
        "train_attx_1h, train_attx, train_prot, train_label = generate_set(train_list[1:])\n",
        "print(train_attx_1h.shape)\n",
        "print(train_attx.shape)\n",
        "print(train_prot.shape)\n",
        "print(train_label.shape)\n",
        "\n",
        "print('\\nGenerating Test Set')\n",
        "test_attx_1h, test_attx, test_prot, test_label = generate_set(test_list[1:])\n",
        "print(test_attx_1h.shape)\n",
        "print(test_attx.shape)\n",
        "print(test_prot.shape)\n",
        "print(test_label.shape)\n",
        "\n",
        "print('\\nGenerating Dev Set')\n",
        "dev_attx_1h, dev_attx, dev_prot, dev_label = generate_set(dev_list[1:])\n",
        "print(dev_attx_1h.shape)\n",
        "print(dev_attx.shape)\n",
        "print(dev_prot.shape)\n",
        "print(dev_label.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIxpUn8-n_zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_prot = train_prot / float(max_prot_len)\n",
        "test_prot = test_prot / float(max_prot_len)\n",
        "dev_prot = dev_prot / float(max_prot_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGg9GcFHLpX9",
        "colab_type": "text"
      },
      "source": [
        "# Model Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0J2ViwLnfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d216217-0991-4fd2-b489-0f241d4cc1ce"
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVDJmw9o2TUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(history):\n",
        "\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history['acc'])\n",
        "  plt.plot(history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Dev'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history['loss'])\n",
        "  plt.plot(history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Dev'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghY1pKVRLi4z",
        "colab_type": "text"
      },
      "source": [
        "# Jaccard-CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKk3b0WdzjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Protein LSTM Embedding Layer\n",
        "prot_input = Input(shape=(max_prot_len,), dtype='float32')\n",
        "\n",
        "# ATTx Convolutional Network\n",
        "attx_input = Input(shape=(max_attx_len, attx_dim, 1), dtype='float32')\n",
        "\n",
        "conv1 = Conv2D(64, \n",
        "               kernel_size=(4, 4), \n",
        "               padding='valid', \n",
        "               strides=1, \n",
        "               activation='relu')(attx_input)\n",
        "\n",
        "pool1 = MaxPooling2D(pool_size=(2, 1), padding='valid')(conv1)\n",
        "\n",
        "conv2 = Conv2D(64, \n",
        "               kernel_size=(5, 1), \n",
        "               padding='valid', \n",
        "               strides=1, \n",
        "               activation='relu')(pool1)\n",
        "\n",
        "pool2 = MaxPooling2D(pool_size=(2, 1), padding='valid')(conv2)\n",
        "\n",
        "conv3 = Conv2D(64, \n",
        "               kernel_size=(5, 1), \n",
        "               padding='valid', \n",
        "               strides=1, \n",
        "               activation='relu')(pool2)\n",
        "\n",
        "drop = Dropout(0.15)(conv3)\n",
        "attx_out = Flatten()(drop)\n",
        "\n",
        "combined = concatenate([prot_input, attx_out])\n",
        "dense1 = Dense(1024, activation='relu', name='Dense1', kernel_regularizer=keras.regularizers.l2(0.002))(combined)\n",
        "dense2 = Dense(1024, activation='relu', name='Dense2', kernel_regularizer=keras.regularizers.l2(0.002))(dense1)\n",
        "pred = Dense(1, activation='sigmoid', name='Dense3', kernel_regularizer=keras.regularizers.l2(0.002))(dense2)\n",
        "\n",
        "model_c = Model(inputs=[prot_input, attx_input], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ukfmua2jDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "adam_c = optimizers.Adam(lr=0.003)\n",
        "model_c.compile(optimizer=adam_c, loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "model_c.summary()\n",
        "plot_model(model_c, show_shapes = True, show_layer_names = False, to_file='model_c.png')\n",
        "\n",
        "\n",
        "filepath=\"model_c_{epoch:02d}_{val_acc:.2f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "                  \n",
        "history_c = model_c.fit([train_prot, train_attx_1h], train_label, epochs=1000, batch_size=2048, \n",
        "                    validation_data = ([dev_prot, dev_attx_1h], dev_label), callbacks=[checkpoint])\n",
        "model_c.save('model_c_long.h5')\n",
        "plot_loss(history_c.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhLAbr_xElpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('model_c_92_0.80.h5')\n",
        "predictions = np.round(model.predict([test_prot, test_attx_1h]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEW6JCAsFmcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_pos = 0\n",
        "true_neg = 0\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "for i in range(len(test_label)):\n",
        "\n",
        "  if (predictions[i] == 1) and (test_label[i] == 1):\n",
        "    true_pos += 1\n",
        "  if (predictions[i] == 1) and (test_label[i] == 0):\n",
        "    false_pos += 1\n",
        "  if (predictions[i] == 0) and (test_label[i] == 0):\n",
        "    true_neg += 1\n",
        "  if (predictions[i] == 0) and (test_label[i] == 1):\n",
        "    false_neg += 1\n",
        "\n",
        "print('True Positive: ', true_pos)\n",
        "print('True Negative: ', true_neg)\n",
        "print('False Positive: ', false_pos)\n",
        "print('False Negative: ', false_neg)\n",
        "\n",
        "print('Accuracy: ', (true_pos + true_neg) / (false_pos + false_neg + true_pos + true_neg))\n",
        "print('Precision: ', (true_pos) / (false_pos + true_pos))\n",
        "print('Recall: ', (true_pos) / (false_neg + true_pos))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}